# **Mini Project Two**

In this project, our group explored how we can visualize the places used in any news articles. Key techniques  were extracting place names from the text using **regex** and a **gazetteer**. We also applied **Named Entity Recognition (NER)** and **Geocoding** to visualize the places on a map. The Following is an oveerview of  main steps  to complete this project.

## Repository structure

our Repository conatins:

-A folder named "Scripts" with all the scripts used for this project.There are Five scripts:"regex_script_final.py","Copy_of_Gaza_NER2_Arsalan_Danish_Ammara_Shahzaib.ipnyb",
"Mapping_Regex_script", "Mapping_NER_script" and "build_gazetteer_script.ipnyb".

-A folder named "gazetteer" containing file named "geonames_gaza_selection", a tsv file containing a selection of places in gaza, a file named coordinates.tsv containing  a coordinate pair for each country in the world, and a tsv file named " NER_gazetteer" conating coordinates of places extracted using NER.

-A folder named "Counts" containing files "ner_counts" and "regex_counts".

-A folder named "Images" containing images for NER built map and regex built map.

-A folder named "HTML" conataining HTML format of NER and Regex built map.

-A folder named "AI_Documentation" containing files with AI documentation of each member.

-A file named "Readme file" with the documentation of our project.


## Extracting Place names from articles (using Regex and gazetteer)

We made a script on python using a gazetteer to identify and tally place names mention across a large artcile corpus. It creates improved regx patterns for better detection
and aggregates mention counts by location and month, outputting results to a TSV file.
Following are the major steps taken to perform this task:

### Adapting the script

Firstly, I adapt the file path so that the script uses the gazetter and a larger corpus in our portfolio repository. Initially, the script only matched the main name(asciiname), but that missed a lot of mentions. So, I build a pattern to match the alternate names as well. For example,
** # extracting ascciname from column 1 **
    asciiname = columns[0]
   ** # Process the row if it has 6 or more columns**
    if asciiname:
        patterns[asciiname] = {"pattern": re.escape(asciiname), "count": 0}

   ** #extracting alternative names from 6th column**
    alternate_names = columns[5].strip()
    if alternate_names:
        alternate_list=alternate_names.split(",") **# Split by commas to handle multiple alternatives**
        for alternate in alternate_list:
            alternate=alternate.strip()
            if alternate:
                patterns[alternate]={"pattern":re.escape(alternate),"count":0} **# escaping speacial characters and adding alternate names if present**

Then, I adapt the script so that it skips the news artciles written before the start of current war (which i set as oct 7th, 2023).It does this by reading the data from each file name and skipping any file before that date. Afterwards, to ttrack how oftene each place is mentioned, i created a dictionary called mentions_per_month. It stores the number of mentions for each place in each month like this:

{
    'Gaza': {'2023-10': 12, '2023-11': 15},
    'Rafah': {'2023-11': 7}
}

### Writing ouputs

At the end, the scripts writes everything to a TSV file called reggex_count.tsv.This file has tnree columns: Placenames, counts and month. Each row tells how many times that place was mentioned.

### Keeping Track with Git

While working, I made sure to regularly add, commit, pull and push my changes with Git Bash:

git add .
git commit -m "message"
git pull origin main
git push origin main

### finalizing

once everything was done, I renamed my scipt to regex_script_final.py, this is the final version of  my script saved in the "scripts" Folder.


## NER with Stanza

### Set up the Notebook
I created a copy of the class notebook and renamed it to
Gaza_NER2_arslan_Shah Zaib_Ammara.ipynb to reflect my group’s names.

### Installation
Then, I installed stanza by using this code: !pip install stanza

### Import library and download language model
Then, I installed and downloaded the English model for Stanza:

### Creating the pipeline
Then, I created a pipeline

### Clone the repository
I cloned FASDH25 folder on google collab.

### Adapted the File Path
I changed the path in the notebook to use the larger corpus from our portfolio repository instead of the session folder.
I made sure the script could loop through all files in the corpus folder.

### Filtered Articles by Date
I added a condition to only extract place names from articles written in January 2024.
To do this, I extracted the date from the filename.

###  Extracted Place Names
I filtered the named entities to extract only locations (GPE, LOC, or FAC).
For every entity that matched, I added it to a dictionary to keep count.

### Cleaned the Entity Names
I cleaned up the place names by removing extra characters like 's and converted everything to lowercase.
For example, I changed "Gaza's" to "Gaza" to avoid duplicates:

###  Saved Results to a TSV File
I wrote the final results to a file called ner_counts.tsv.tsv, with two columns.

### Keeping Track with Git
I downloaded the notebook as an .ipynb file, moved it to the scripts folder, and renamed it properly.
Then, I committed and pushed the changes to GitHub By using
git add.
git commit -m "Added final NER script for Task 2B"
git pull origin main
git push origin main


## Building Gazetteer for the NER places

### Introducing the project
This part of the project creates a gazetteer of named entities(places) extracted through Named Entity recognition (NER). Using the [Geonames API](https://www.geonames.org/export/web-services.html, we retrieved the latitude and longitude of each place mentioned in the 'ner_counts.tsv' file and stored the output in 'NER_gazetteer.tsv'.

### files and folders

ner_counts.tsv(TSV file with input of place names and NER counts)
gazetteer/NER_gazetteer.tsv (final output with coordinates)
build_gazetteer.ipynb(scrypt for reading and importing, geocoding and writing the gazetteer)
gazetteers/ (foldder containing the output file in FASDH25-protfolio2 )


## steps

The script reads ner_counts.tsv line by line which was taken from the tsv file which contains two columns. 
for files reading we used "with open(...)"
loop through lines maually and use .split('\t') for TSV removing duplicates.
the 'get_coordinates()' function uses the Geonames to fetch latitude and longitude for each place
if no coordinates were found for a place, the function returned "NA" for both latitude and longitude.
Such places coordinates were added to the gazetteer tsv file manually.
The final output, 'NER_gazetteer.tsv', is written to the NER_gazetteer.tsv in the folder gazetteers in the FASDH25-portfolio2

Collect place names into simple alternate_list

## Maually looked up files 
Some of the places coordinates were missing in the gazetter tsv labelling NA, so i had to manually add the coordinates from google and also from chatgpt. The other problem i encountered was to remove names of people, news agencies, and also groups belongings to different organization.
I added the NA values manually in spreatsheet and below is the list of place names:
Dahiyeh
Shujayea
Jama'A Islamiya coordinates
Mezzeh
Berui
Philadelphi corridor
Balakhiyah
Zawiya al-Ahmaddiya
Dahieh
Taalabaya
Rmeish
Panjgur
Zar'it
Margaliot
Yiron


## Mapping the regex extracted place names

This task visualizes the frequency per month of places extracted using regex.An animated and a static map is created with plotly express displays how often each place was mentioned over time.
Following are the major steps taken to perform this task:

### Data Preparation

- loaded geonames_gaza_selection.tsv containing  place names with their geographic coordinates (latitude and longitude).
- Loaded  regex_counts.tsv containing the frequency of mentions of those place names, extracted using regex, along with the corresponding month.
- Merged both datasets using the common column asciiname to combine geographic and frequency data into a single DataFrame. Note: **I adapted the python script for regex_counts to name the column with place names as asciiname, so that there will be no issue while merging the both files**.

### Visualization

- used plotly.express.scatter_geo() to create a world map showing place name frequencies.
- each dot represents a place; its size and color reflect how frequently it was mentioned.
- Added animation_frame="month" to visualize changes over time (one frame per month)
- customized the map appearance

### Design experiments

- Tried both scatter_geo and scatter_mapbox; chose scatter_geo because it worked well with the small geographic region (Gaza area) and required no additional API keys.
- Chose natural earth projection for a clean, minimalistic look that highlights data without clutter.

### Saving the output
- saved the map as: regex_map.html for web viewing and interaction, and as regex_map.png for a static snapshot of the full map.


## Mapping the NER extracted place names
<<<<<<< HEAD
So firstly, we used the script of mapping regex count because it was quite similar. We changed the file paths and and name of the common column. then, we executed the code but it was showing multiple errors continuously for two days. We consulted our professor Mathew he helped us with this issue. He told us that our longitudes and latitudes were in strings that is why were getting multiple errors. He changed the longitudes and latitudes to float instead of strings, and then we didnot get any error. But in our map the frequencies were scattered all over the map, tho we got 326 articles after 2024 on our NER file. 





=======
>>>>>>> 94910def470231f0f92f0f3111637a4633982292

## Analysis of Regex+Gazetteer techniques

The regex and gazetteer method works effectively when using complete term lists and well-designed patterns, providing accurate results without requiring machine learning or labeled data. However, this approach cannot detect names missing from the list or alternative spellings, and is unable to learn new patterns independently, functioning only within explicitly defined rules.

## Analysis of NER+Stanza Method

NER + Stanza method is organized and structured way for extracting location names from the articles . It produces accurate and well-organized outputs, particularly when working with large amounts of unstructured data, is one of its main advantages. The use of modern NLP Stanza, produces excellent named entity recognition, especially for geographical names. Additionally, you can improve the outcomes even more with AI support. Additionally, the approach is easy to implement into broader data workflows because it is accessible with other tools, such as Python, and is quite user-friendly. However, there are also significant drawbacks. There is a chance that someone else might review or even alter your NER output files if they are kept on sharing systems like Google Drive without the appropriate access permissions. This presents significant issues with data security and privacy, particularly when handling sensitive material. Moreover, tasks 2B and 3 were performed on two different NER files, which created challenges when trying to work them together. This fragmentation of data made it difficult to integrate and compare results efficiently.

## Images of the Maps

![Images](C:\Users\GB\Downloads\FASDH25-portfolio2\Images/ner_map.png)

![Images](C:\Users\GB\Downloads\FASDH25-portfolio2\Images/regex_map.png)

## Comparing Maps built by NER and Regex
The two maps generated from regex and NER data for January 2024 show different viewpoints on how placename mentions are distributed geographically. The NER-based map presents a wide regional view. The placenames appear across the Middle East, North Africa, and parts of Europe. This method seems to cast a wider network. This is capturing mentions from a range of sources and locations from the densely highlighted area around Israel and Palestine to scattered mentions across Turkey, Iran, and even parts of North and West Africa. The color intensity, peaking around mentions, suggests a balanced yet widespread frequency of place references. This reflects how NLP models recognize named entities across various backgrounds.

In comparison, the regex-based map zooms in with deep focus.  This highlights a dense concentration of placename mentions almost exclusively within the Gaza Strip. This map reveals a hyperlocal pattern.  with the count scale shooting above 4,500 mentions in certain hotspots. It suggests that regex-based extraction, while narrower in scope, is highly effective at identifying frequent references to specific areas. Particularly in contexts like news reporting on conflict zones. This approach doesn’t pick up the broader regional picture but stands out at focusing on areas of high attention and repeated mention. Together, the two maps tell different stories: one of wide-ranging attention and another of intense, localized focus. The localized focus may be due to the current genocide in Gaza. 









## Critical Analysis of the Project

 One of the main challenges we faced was our limited experience with coding, combined with the short time frame for the project. With more time and additional practice, we believe we could have completed the project more effectively and with greater confidence. Additionally, some of the instructions provided were not detailed enough. In certain cases, we encountered unexpected errors that required manual corrections even after following the instructions precisely. Clearer guidance or more examples could have facilitated troubleshooting these issues more efficiently. Overall, the project was practical, providing valuable hands-on experience and encouraging out-of-the-box thinking, though the time available was a limiting factor. 


